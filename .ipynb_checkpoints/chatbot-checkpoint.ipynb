{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXp1J6eg4qEh",
    "outputId": "87cb0c4e-bc5e-4a7e-9cbc-2c2f8d43455f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "vHV3nzWw5DJw"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXED-OYz4-1h",
    "outputId": "2c0b7956-5870-4d0a-d7da-842bd20b982f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie_titles_metadata.txt', 'movie_characters_metadata.txt', '.DS_Store', 'movie_lines.txt', 'raw_script_urls.txt', 'chameleons.pdf', 'movie_conversations.txt', 'README.txt']\n"
     ]
    }
   ],
   "source": [
    "zip_path = '/content/drive/MyDrive/Data/movie_corpus.zip'\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/movie_corpus')\n",
    "\n",
    "# Path to the extracted files\n",
    "extracted_path = '/content/movie_corpus'\n",
    "print(os.listdir(extracted_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_4h-NeuJMHFz"
   },
   "outputs": [],
   "source": [
    "# Load movie lines\n",
    "lines_path = os.path.join(extracted_path, 'movie_lines.txt')\n",
    "conversations_path = os.path.join(extracted_path, 'movie_conversations.txt')\n",
    "\n",
    "# Read movie_lines.txt\n",
    "# 'sep' is the seperator by which data is seperated in our txt file\n",
    "# 'header= None' means that there is no header row in the txt file by default\n",
    "# 'names' list gives us the option to specify headers on our own\n",
    "lines = pd.read_csv(lines_path, sep='\\+\\+\\+\\$\\+\\+\\+', header=None, engine='python', names=['lineID', 'characterID', 'movieID', 'character', 'text'], encoding='latin-1')\n",
    "\n",
    "# Read movie_conversations.txt\n",
    "conversations = pd.read_csv(conversations_path, sep='\\+\\+\\+\\$\\+\\+\\+', header=None, engine='python', names=['character1ID', 'character2ID', 'movieID', 'utteranceIDs'], encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XDxJpYm4M5OW"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map each line's ID to its text\n",
    "id2line = {line.lineID: line.text for line in lines.itertuples()}\n",
    "\n",
    "# This will be the sample output:\n",
    "# {\n",
    "#    1: 'Hello, how are you?',\n",
    "#    2: \"I'm fine, thank you.\",\n",
    "#    3: 'Good morning!',\n",
    "#    4: 'Hi there.'\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "TiJh2cHhM7aR"
   },
   "outputs": [],
   "source": [
    "# Extract conversations\n",
    "# This will be a list of lists, where each inner list represents a conversation\n",
    "conversations_data = []\n",
    "\n",
    "for conv in conversations.itertuples():\n",
    "    # Convert the string representation of list to a list of strings\n",
    "    #utteranceID is the column in dataframe that is a list of string of line IDs\n",
    "    utterance_ids = ast.literal_eval(conv.utteranceIDs)\n",
    "    # Get the text corresponding to each line ID\n",
    "    conv_texts = [id2line.get(uid, '') for uid in utterance_ids]\n",
    "    # Append the conversation to the list\n",
    "    conversations_data.append(conv_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "KmH1lTGkNp3h"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "     # Replace contractions and common abbreviations with full forms\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    # Remove leading and trailing spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing to each line in the conversations\n",
    "preprocessed_conversations = []\n",
    "for conv in conversations_data:\n",
    "    # Preprocess each line in the conversation\n",
    "    preprocessed_conversations.append([preprocess_text(line) for line in conv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4pA_8he0XVjb"
   },
   "outputs": [],
   "source": [
    "\n",
    "input_texts = []\n",
    "\n",
    "target_texts = []\n",
    "#Iterating through each conversation that is preprocessed\n",
    "for conv in preprocessed_conversations:\n",
    "    #Iterating through each line in the conversation\n",
    "    for i in range(len(conv) - 1):\n",
    "        #Appending the input and target texts\n",
    "        input_texts.append(conv[i]) #input text\n",
    "        target_texts.append(conv[i + 1])  #target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMwRWb4zYHEt"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the input and target texts\n",
    "tokenizer = Tokenizer()\n",
    "#Fit the tokenizer on the input and target texts\n",
    "#This will create the vocabulary of words used in the texts\n",
    "#This will split the words\n",
    "#This will also assign a unique integer to each word\n",
    "#This will also count the frequency of each word\n",
    "# 1: i (i occured most for example) 2: am (am occured 2nd most for example)\n",
    "tokenizer.fit_on_texts(input_texts + target_texts)\n",
    "\n",
    "# Convert the input texts to sequences of integers\n",
    "# 'hello how are you' becomes [6, 7, 8, 5]\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "\n",
    "# Convert the target texts to sequences of integers\n",
    "# 'i am fine thank you' becomes [1, 2, 3, 4, 5].\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "\n",
    "#finding max of both sequences\n",
    "max_length = max(max(len(seq) for seq in input_sequences), max(len(seq) for seq in target_sequences))\n",
    "#pad zeros in small length input sequences at the end\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding='post')\n",
    "#pad zeros in small length target sequences at the end\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADU-MU4Vg3t2"
   },
   "outputs": [],
   "source": [
    "#From here on split data into training and testing data\n",
    "# chose network of the choice LSTM, Transformer"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
