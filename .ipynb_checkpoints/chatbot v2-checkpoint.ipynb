{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzL9poEXz5yU"
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXp1J6eg4qEh",
    "outputId": "2118a7c4-701f-44c9-988e-0246fd93cc29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vHV3nzWw5DJw"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXED-OYz4-1h",
    "outputId": "3a931636-e9c5-4be6-94a3-51c268b06d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie_titles_metadata.txt', 'movie_characters_metadata.txt', '.DS_Store', 'movie_lines.txt', 'raw_script_urls.txt', 'chameleons.pdf', 'movie_conversations.txt', 'README.txt']\n"
     ]
    }
   ],
   "source": [
    "# zip_path = '/content/drive/MyDrive/Data/movie_corpus.zip' # Kasha~af\n",
    "zip_path = '/content/drive/MyDrive/Internship/ChatBot.zip' # Ahmed S.\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('/content/movie_corpus')\n",
    "\n",
    "# Path to the extracted files\n",
    "extracted_path = '/content/movie_corpus'\n",
    "print(os.listdir(extracted_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_4h-NeuJMHFz"
   },
   "outputs": [],
   "source": [
    "# Load movie lines\n",
    "lines_path = os.path.join(extracted_path, 'movie_lines.txt')\n",
    "conversations_path = os.path.join(extracted_path, 'movie_conversations.txt')\n",
    "\n",
    "# Read movie_lines.txt\n",
    "# 'sep' is the seperator by which data is seperated in our txt file\n",
    "# 'header= None' means that there is no header row in the txt file by default\n",
    "# 'names' list gives us the option to specify headers on our own\n",
    "lines = pd.read_csv(lines_path, sep='\\+\\+\\+\\$\\+\\+\\+', header=None, engine='python', names=['lineID', 'characterID', 'movieID', 'character', 'text'], encoding='latin-1')\n",
    "\n",
    "# Read movie_conversations.txt\n",
    "conversations = pd.read_csv(conversations_path, sep='\\+\\+\\+\\$\\+\\+\\+', header=None, engine='python', names=['character1ID', 'character2ID', 'movieID', 'utteranceIDs'], encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XDxJpYm4M5OW"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map each line's ID to its text\n",
    "id2line = {line.lineID: line.text for line in lines.itertuples()}\n",
    "\n",
    "# This will be the sample output:\n",
    "# {\n",
    "#    1: 'Hello, how are you?',\n",
    "#    2: \"I'm fine, thank you.\",\n",
    "#    3: 'Good morning!',\n",
    "#    4: 'Hi there.'\n",
    "# }\n",
    "\n",
    "# Fixed an error, there was an extra space after each lineID\n",
    "id2line_new = {key.replace(\" \", \"\") if \" \" in key else key: value for key, value in id2line.items()}\n",
    "id2line = id2line_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TiJh2cHhM7aR"
   },
   "outputs": [],
   "source": [
    "# Extract conversations\n",
    "conversations_data = []\n",
    "\n",
    "for conv in conversations.itertuples():\n",
    "    # Convert the string representation of list to a list of strings\n",
    "    utterance_ids = ast.literal_eval(conv.utteranceIDs)\n",
    "    # Get the text corresponding to each line ID\n",
    "    conv_texts = [id2line.get(uid, '') for uid in utterance_ids if uid in id2line]\n",
    "    if conv_texts:  # Only add non-empty conversations\n",
    "        conversations_data.append(conv_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KmH1lTGkNp3h"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to str (for some reason the text was being considered floatðŸ˜’)\n",
    "    text = str(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "     # Replace contractions and common abbreviations with full forms\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    # Remove leading and trailing spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing to each line in the conversations\n",
    "preprocessed_conversations = []\n",
    "for conv in conversations_data:\n",
    "    # Preprocess each line in the conversation\n",
    "    preprocessed_conversations.append([preprocess_text(line) for line in conv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4pA_8he0XVjb"
   },
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "\n",
    "target_texts = []\n",
    "#Iterating through each conversation that is preprocessed\n",
    "for conv in preprocessed_conversations:\n",
    "    #Iterating through each line in the conversation\n",
    "    for i in range(len(conv) - 1):\n",
    "        #Appending the input and target texts\n",
    "        input_texts.append(conv[i]) #input text\n",
    "        target_texts.append(conv[i + 1])  #target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YMwRWb4zYHEt"
   },
   "outputs": [],
   "source": [
    "# Tokenize the input and target texts\n",
    "tokenizer = Tokenizer()\n",
    "#Fit the tokenizer on the input and target texts\n",
    "#This will create the vocabulary of words used in the texts\n",
    "#This will split the words\n",
    "#This will also assign a unique integer to each word\n",
    "#This will also count the frequency of each word\n",
    "# 1: i (i occured most for example) 2: am (am occured 2nd most for example)\n",
    "tokenizer.fit_on_texts(input_texts + target_texts)\n",
    "\n",
    "# Convert the input texts to sequences of integers\n",
    "# 'hello how are you' becomes [6, 7, 8, 5]\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "\n",
    "# Convert the target texts to sequences of integers\n",
    "# 'i am fine thank you' becomes [1, 2, 3, 4, 5].\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "\n",
    "#finding max of both sequences\n",
    "max_length = max(max(len(seq) for seq in input_sequences), max(len(seq) for seq in target_sequences))\n",
    "#pad zeros in small length input sequences at the end\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding='post')\n",
    "#pad zeros in small length target sequences at the end\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ADU-MU4Vg3t2"
   },
   "outputs": [],
   "source": [
    "#From here on split data into training and testing data\n",
    "# chose network of the choice LSTM, Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsCAwYXTrYy9"
   },
   "source": [
    "# **Natural Language Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "an7RJv8V-Siq"
   },
   "source": [
    "NLU is a subfield of AI that focuses on enabling computers to understand the meaning and intent behind human language.\n",
    "\n",
    "NLU is used in this project to:\n",
    "1.  Understand the user's goals and intentions.\n",
    "2.  Extract relevant information from user input.\n",
    "3.  Respond in a way that aligns with the identified intent and context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XnIVo_9Tu_zg"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruQH20yIudPz"
   },
   "source": [
    "## **Intent Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gVXCobTCuglG"
   },
   "outputs": [],
   "source": [
    "INTENT_PATH = '/content/drive/MyDrive/Internship/intents.json'\n",
    "\n",
    "# Open the file and load the JSON data into the 'intents'\n",
    "with open(INTENT_PATH) as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "# Extract the 'intents' key from the loaded JSON data\n",
    "intents = intents['intents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_r7SEHTpwnWm"
   },
   "outputs": [],
   "source": [
    "# Intents.json file has various intent dictionaries each following the following structure\n",
    "# {\n",
    "#     \"tag\": \"unique_intent_identifier\",  # e.g., \"greeting\", \"goodbye\"\n",
    "#     \"patterns\": [ List of user phrases that trigger this intent ],\n",
    "#     \"responses\": [ List of possible responses the chatbot can give ],\n",
    "#     \"context_set\": \"\"   used for context-aware chatbots\n",
    "# }\n",
    "\n",
    "\n",
    "def get_intent(user_input):\n",
    "    \"\"\"\n",
    "    Searches for a matching intent based on user input.\n",
    "\n",
    "    This function iterates through all intents in the loaded `intents` list.\n",
    "    For each intent, it checks if any patterns (user phrases) match the lowercased user input.\n",
    "    If a match is found, the corresponding intent dictionary is returned. Otherwise, the function returns None.\n",
    "\n",
    "    Args:   user_input (str): The user's input phrase.\n",
    "\n",
    "    Returns:  dict or None: The matching intent dictionary if found, otherwise None.\n",
    "    \"\"\"\n",
    "\n",
    "    for intent in intents:\n",
    "        for pattern in intent['patterns']:\n",
    "            if pattern.lower() in user_input.lower():\n",
    "                return intent\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_response(intent):\n",
    "    \"\"\"\n",
    "    Selects a random response from the provided intent.\n",
    "\n",
    "    This function assumes the `intent` dictionary has a \"responses\" key containing a list of possible chatbot responses.\n",
    "    It randomly chooses one of these responses and returns it.\n",
    "\n",
    "    Args:   intent (dict): The intent dictionary containing user patterns and responses.\n",
    "\n",
    "    Returns:  str: A random response chosen from the intent's \"responses\" list.\n",
    "    \"\"\"\n",
    "    return random.choice(intent['responses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0UCaEzstKP6"
   },
   "source": [
    "## **Name Entity Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "37xVi1Cmrh48"
   },
   "outputs": [],
   "source": [
    "# Load the spaCy English small model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_entities(user_input):\n",
    "    \"\"\"\n",
    "    Extracts named entities from user input using spaCy.\n",
    "\n",
    "    This function takes a string of user input and processes it using the loaded spaCy model (`nlp`).\n",
    "    The model identifies and classifies named entities within the text. The function then iterates through the identified entities,\n",
    "    extracting their text and label (type of entity) and returning a list of tuples containing these values.\n",
    "\n",
    "    Args:   user_input (str): The user's input text.\n",
    "\n",
    "    Returns:  list: A list of tuples where each tuple contains (entity_text, entity_label).\n",
    "    \"\"\"\n",
    "\n",
    "    # for example\n",
    "    # User Input: Barack Obamma was the president of United States of America\n",
    "    # Entities:\n",
    "    #         1) Barack Obamma -> Person\n",
    "    #         2) United States of America -> GPE (GeoPolitical Entity)\n",
    "\n",
    "    doc = nlp(user_input)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjqfkzyAtQK_"
   },
   "source": [
    "## **Context Management**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YwcFYQsMwvcQ"
   },
   "outputs": [],
   "source": [
    "current_context = \"\"\n",
    "\n",
    "# Getter and Setter functions for context\n",
    "def get_context():\n",
    "    return current_context\n",
    "\n",
    "def set_context(context):\n",
    "    global current_context\n",
    "    current_context = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XI-iFbtPrmwk"
   },
   "outputs": [],
   "source": [
    "# Dictionary to store context information for intents that will be used to set context\n",
    "intent_context_map = {}\n",
    "for intent in intents:\n",
    "    if intent['context_set']:\n",
    "        intent_context_map[intent['tag']] = intent['context_set']\n",
    "    else:\n",
    "        intent_context_map[intent['tag']] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rMB9XiVEvSDf"
   },
   "outputs": [],
   "source": [
    "def handle_input(user_input):\n",
    "    \"\"\"\n",
    "    Processes user input, returning a response and extracted entities.\n",
    "\n",
    "    This function takes the user's input as a string. It first calls `get_intent` to identify the intent (purpose)\n",
    "    behind the user's input based on the loaded intents and their patterns.\n",
    "\n",
    "    If an intent is found:\n",
    "        - `get_response` is called to retrieve a random response from the corresponding intent's list of responses.\n",
    "        - `extract_entities` is used to identify and extract named entities from the user's input using the spaCy library.\n",
    "        - The `intent_context_map` is used to look up the context associated with the identified intent tag. If context exists, the `set_context`\n",
    "          function is called to set the current conversational context. Otherwise, the context is set to None.\n",
    "\n",
    "    If no intent is found, a default response indicating confusion is returned, and an empty list of entities is provided.\n",
    "\n",
    "    Args:   user_input (str): The user's input text.\n",
    "\n",
    "    Returns:  tuple: A tuple containing (response_text, list_of_entities).\n",
    "    \"\"\"\n",
    "\n",
    "    intent = get_intent(user_input)\n",
    "\n",
    "    if intent:\n",
    "        response = get_response(intent)\n",
    "        entities = extract_entities(user_input)\n",
    "        context = intent_context_map[intent['tag']]\n",
    "        if context:\n",
    "            set_context(context)\n",
    "        else:\n",
    "            set_context(None)\n",
    "    else:\n",
    "        response = \"I'm not quite sure what you mean by that. Can you try explaining it in a different way?\"\n",
    "        entities = []\n",
    "\n",
    "    return response, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aC-fjxYVx3tM",
    "outputId": "b8706434-22fb-4ef5-e454-8d66b44a9ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Hello\n",
      "Bot: Hello!\n",
      "Entities: []\n",
      "Context: None\n",
      "\n",
      "==================================================\n",
      "\n",
      "You: donald trump made america great?\n",
      "Bot: I'm not quite sure what you mean by that. Can you try explaining it in a different way?\n",
      "Entities: []\n",
      "Context: None\n",
      "\n",
      "==================================================\n",
      "\n",
      "You: bye\n",
      "Bot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Testing NLU\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"bye\":\n",
    "        response, _ = handle_input(user_input)\n",
    "        print(\"Bot:\", response)\n",
    "        break\n",
    "\n",
    "    response, entities = handle_input(user_input)\n",
    "    print(\"Bot:\", response)\n",
    "    print('Entities:', entities)\n",
    "    print(\"Context:\", get_context())\n",
    "    print('\\n'+'='*50+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FRzb_0w_HGA"
   },
   "source": [
    "# **Response Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3djZi0V_cDs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
