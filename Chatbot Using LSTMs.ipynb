{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4fa148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec78341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read movie_lines.txt\n",
    "lines = pd.read_csv('Dataa/movie_lines.txt', sep='\\+\\+\\+\\$\\+\\+\\+', header=None, engine='python', names=['lineID', 'characterID', 'movieID', 'character', 'text'], encoding='latin-1')\n",
    "\n",
    "# Read movie_conversations.txt\n",
    "conversations = pd.read_csv('Dataa/movie_conversations.txt', sep='\\+\\+\\+\\$\\+\\+\\+', header=None, engine='python', names=['character1ID', 'character2ID', 'movieID', 'utteranceIDs'], encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ae52f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lineID characterID movieID character           text\n",
      "0  L1045          u0      m0    BIANCA    They do not!\n",
      "  character1ID character2ID movieID                       utteranceIDs\n",
      "0          u0           u2      m0    ['L194', 'L195', 'L196', 'L197']\n",
      "304713\n",
      "83097\n"
     ]
    }
   ],
   "source": [
    "print(lines.head(1))\n",
    "print(conversations.head(1))\n",
    "print(len(lines))\n",
    "print(len(conversations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace04a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map each line's ID to its text\n",
    "id2line = {line.lineID: line.text for line in lines.itertuples()}\n",
    "\n",
    "# This will be the sample output:\n",
    "# {\n",
    "#    1: 'Hello, how are you?',\n",
    "#    2: \"I'm fine, thank you.\",\n",
    "#    3: 'Good morning!',\n",
    "#    4: 'Hi there.'\n",
    "# }\n",
    "\n",
    "# Fixed an error, there was an extra space after each lineID\n",
    "id2line_new = {key.replace(\" \", \"\") if \" \" in key else key: value for key, value in id2line.items()}\n",
    "id2line = id2line_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f1ff687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract conversations\n",
    "conversations_data = []\n",
    "\n",
    "for conv in conversations.itertuples():\n",
    "    # Convert the string representation of list to a list of strings\n",
    "    utterance_ids = ast.literal_eval(conv.utteranceIDs)\n",
    "    # Get the text corresponding to each line ID\n",
    "    conv_texts = [id2line.get(uid, '') for uid in utterance_ids if uid in id2line]\n",
    "    if conv_texts:  # Only add non-empty conversations\n",
    "        conversations_data.append(conv_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55804172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to str (for some reason the text was being considered float😒)\n",
    "    text = str(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and non-alphabetic characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "     # Replace contractions and common abbreviations with full forms\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    # Remove leading and trailing spaces\n",
    "    return text.strip()\n",
    "# Apply preprocessing to each line in the conversations\n",
    "preprocessed_conversations = []\n",
    "for conv in conversations_data:\n",
    "    # Preprocess each line in the conversation\n",
    "    preprocessed_conversations.append([preprocess_text(line) for line in conv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d93d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "#Iterating through each conversation that is preprocessed\n",
    "for conv in preprocessed_conversations:\n",
    "    #Iterating through each line in the conversation\n",
    "    for i in range(len(conv) - 1):\n",
    "        #Appending the input and target texts\n",
    "        input_texts.append(conv[i]) #input text\n",
    "        target_texts.append(conv[i + 1])  #target text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "376fc75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the start and end tags to the sentence to know where to stop generating the text\n",
    "target_texts = [\"<start> \" + text + \" <end>\" for text in target_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb5bb56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input and target texts\n",
    "tokenizer = Tokenizer()\n",
    "#Fit the tokenizer on the input and target texts\n",
    "#This will create the vocabulary of words used in the texts\n",
    "#This will split the words\n",
    "#This will also assign a unique integer to each word\n",
    "#This will also count the frequency of each word\n",
    "# 1: i (i occured most for example) 2: am (am occured 2nd most for example)\n",
    "tokenizer.fit_on_texts(input_texts + target_texts)\n",
    "\n",
    "# Convert the input texts to sequences of integers\n",
    "# 'hello how are you' becomes [6, 7, 8, 5]\n",
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "\n",
    "# Convert the target texts to sequences of integers\n",
    "# 'i am fine thank you' becomes [1, 2, 3, 4, 5].\n",
    "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "\n",
    "#finding max of both sequences\n",
    "#max_length = max(max(len(seq) for seq in input_sequences), max(len(seq) for seq in target_sequences))\n",
    "#pad zeros in small length input sequences at the end\n",
    "max_length = 70\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding='post')\n",
    "#pad zeros in small length target sequences at the end\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef387825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From here on split data into training and testing data\n",
    "# chose network of the choice LSTM, Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ec49b",
   "metadata": {},
   "source": [
    "# **Natural Language Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663f9cb",
   "metadata": {},
   "source": [
    "NLU is a subfield of AI that focuses on enabling computers to understand the meaning and intent behind human language.\n",
    "\n",
    "NLU is used in this project to:\n",
    "1.  Understand the user's goals and intentions.\n",
    "2.  Extract relevant information from user input.\n",
    "3.  Respond in a way that aligns with the identified intent and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89112ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85542225",
   "metadata": {},
   "source": [
    "# Intent Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a4d2006",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTENT_PATH = 'Dataa/intents.json'\n",
    "\n",
    "# Open the file and load the JSON data into the 'intents'\n",
    "with open(INTENT_PATH) as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "# Extract the 'intents' key from the loaded JSON data\n",
    "intents = intents['intents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b4615b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intents.json file has various intent dictionaries each following the following structure\n",
    "# {\n",
    "#     \"tag\": \"unique_intent_identifier\",  # e.g., \"greeting\", \"goodbye\"\n",
    "#     \"patterns\": [ List of user phrases that trigger this intent ],\n",
    "#     \"responses\": [ List of possible responses the chatbot can give ],\n",
    "#     \"context_set\": \"\"   used for context-aware chatbots\n",
    "# }\n",
    "\n",
    "\n",
    "def get_intent(user_input):\n",
    "    \"\"\"\n",
    "    Searches for a matching intent based on user input.\n",
    "\n",
    "    This function iterates through all intents in the loaded `intents` list.\n",
    "    For each intent, it checks if any patterns (user phrases) match the lowercased user input.\n",
    "    If a match is found, the corresponding intent dictionary is returned. Otherwise, the function returns None.\n",
    "\n",
    "    Args:   user_input (str): The user's input phrase.\n",
    "\n",
    "    Returns:  dict or None: The matching intent dictionary if found, otherwise None.\n",
    "    \"\"\"\n",
    "\n",
    "    for intent in intents:\n",
    "        for pattern in intent['patterns']:\n",
    "            if pattern.lower() in user_input.lower():\n",
    "                return intent\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_response(intent):\n",
    "    \"\"\"\n",
    "    Selects a random response from the provided intent.\n",
    "\n",
    "    This function assumes the `intent` dictionary has a \"responses\" key containing a list of possible chatbot responses.\n",
    "    It randomly chooses one of these responses and returns it.\n",
    "\n",
    "    Args:   intent (dict): The intent dictionary containing user patterns and responses.\n",
    "\n",
    "    Returns:  str: A random response chosen from the intent's \"responses\" list.\n",
    "    \"\"\"\n",
    "    return random.choice(intent['responses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9739ad5",
   "metadata": {},
   "source": [
    "# **Name Entity Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e94aaf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy English small model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_entities(user_input):\n",
    "    \"\"\"\n",
    "    Extracts named entities from user input using spaCy.\n",
    "\n",
    "    This function takes a string of user input and processes it using the loaded spaCy model (`nlp`).\n",
    "    The model identifies and classifies named entities within the text. The function then iterates through the identified entities,\n",
    "    extracting their text and label (type of entity) and returning a list of tuples containing these values.\n",
    "\n",
    "    Args:   user_input (str): The user's input text.\n",
    "\n",
    "    Returns:  list: A list of tuples where each tuple contains (entity_text, entity_label).\n",
    "    \"\"\"\n",
    "\n",
    "    # for example\n",
    "    # User Input: Barack Obamma was the president of United States of America\n",
    "    # Entities:\n",
    "    #         1) Barack Obamma -> Person\n",
    "    #         2) United States of America -> GPE (GeoPolitical Entity)\n",
    "\n",
    "    doc = nlp(user_input)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a658274",
   "metadata": {},
   "source": [
    "# **Context Management**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "909a6df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_context = \"\"\n",
    "\n",
    "# Getter and Setter functions for context\n",
    "def get_context():\n",
    "    return current_context\n",
    "\n",
    "def set_context(context):\n",
    "    global current_context\n",
    "    current_context = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a3ae58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store context information for intents that will be used to set context\n",
    "intent_context_map = {}\n",
    "for intent in intents:\n",
    "    if intent['context_set']:\n",
    "        intent_context_map[intent['tag']] = intent['context_set']\n",
    "    else:\n",
    "        intent_context_map[intent['tag']] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cedf9209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_input(user_input):\n",
    "    \"\"\"\n",
    "    Processes user input, returning a response and extracted entities.\n",
    "\n",
    "    This function takes the user's input as a string. It first calls `get_intent` to identify the intent (purpose)\n",
    "    behind the user's input based on the loaded intents and their patterns.\n",
    "\n",
    "    If an intent is found:\n",
    "        - `get_response` is called to retrieve a random response from the corresponding intent's list of responses.\n",
    "        - `extract_entities` is used to identify and extract named entities from the user's input using the spaCy library.\n",
    "        - The `intent_context_map` is used to look up the context associated with the identified intent tag. If context exists, the `set_context`\n",
    "          function is called to set the current conversational context. Otherwise, the context is set to None.\n",
    "\n",
    "    If no intent is found, a default response indicating confusion is returned, and an empty list of entities is provided.\n",
    "\n",
    "    Args:   user_input (str): The user's input text.\n",
    "\n",
    "    Returns:  tuple: A tuple containing (response_text, list_of_entities).\n",
    "    \"\"\"\n",
    "\n",
    "    intent = get_intent(user_input)\n",
    "    if intent:\n",
    "        response = get_response(intent)\n",
    "        entities = extract_entities(user_input)\n",
    "        context = intent_context_map[intent['tag']]\n",
    "        if context:\n",
    "            set_context(context)\n",
    "        else:\n",
    "            set_context(None)\n",
    "    else:\n",
    "        response = retrieve_response(user_input)\n",
    "        if(response):\n",
    "            response = response\n",
    "            entities = extract_entities(user_input)\n",
    "        else:\n",
    "            response = generate_reponse(user_input)\n",
    "            entities = extract_entities(user_input)\n",
    "\n",
    "    return response, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d43b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c13b5036",
   "metadata": {},
   "source": [
    "# Response Generator\n",
    "<ul>\n",
    "<li> 1. Rule Based Reponse </li>\n",
    "    - As Rule Based is already define which is in the form if intent file\n",
    "<li> 2. Retrieval Based Reponse </li>\n",
    "    - Retrieval Based is Implemented below where I have to retrieve the answer from the already given data like\n",
    "    - input sequences, targeted sequences\n",
    "<li> 3. Generative Reponse </li>\n",
    "    - For Generative Response we will train a model using transformer givin below\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b728ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve_response(user_input):\n",
    "    user_in = tokenizer.texts_to_sequences([user_input])\n",
    "    input_seq = pad_sequences(user_in, maxlen=max_length, padding='post')\n",
    "    similarities = cosine_similarity(input_seq, input_sequences)[0]\n",
    "    \n",
    "    similar_indices = np.where(similarities > 0.95)[0]\n",
    "    if len(similar_indices) > 0:\n",
    "        most_similar_index = similar_indices[0]  # Take the first one above threshold\n",
    "        output_sequence = target_sequences[most_similar_index]\n",
    "        output_text = tokenizer.sequences_to_texts([output_sequence])[0]\n",
    "        return output_text\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6b80228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In reponse Generation we have three type to generate out response \n",
    "# 1. Rule-Based in this we have to get user input and Check it in the intent file if found then it will be return to the as an\n",
    "# answer by the bot if not found in the Rule-Based then switch to second type\n",
    "# 2. Retrivel Based in this type we have to get input from the user and check it's simililarity with target_sequence as already\n",
    "# define and if found or we will give it as an answer here we will also define a score from above that score we will get the result and \n",
    "# below that score we will then switch to third type\n",
    "# 3. In this type a Generative reponse will be generate based on the trained model it might be RNN or transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a94bd",
   "metadata": {},
   "source": [
    "# Model Training Using (seq2seq) LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb139cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting data to training, validation, testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "input_train, input_test, target_train, target_test = train_test_split(input_sequences, target_sequences, test_size=0.3, random_state=42)\n",
    "input_train, input_val, target_train, target_val = train_test_split(input_train, target_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7f908ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "# Define the encoder\n",
    "encoder_inputs = Input(shape=(max_length,))\n",
    "encoder_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64)(encoder_inputs)\n",
    "encoder_lstm = LSTM(128, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the decoder\n",
    "decoder_inputs = Input(shape=(max_length,))\n",
    "decoder_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64)(decoder_inputs)\n",
    "decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# encoder_input_data & decoder_input_data into decoder_target_data\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce7c70cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  128/15513\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:37:57\u001b[0m 616ms/step - accuracy: 0.7947 - loss: 6.1987"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m target_val_shifted[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m target_val[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     10\u001b[0m     [input_train, target_train], target_train_shifted,\n\u001b[0;32m     11\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     12\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     13\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m([input_val, target_val], target_val_shifted)\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1506\u001b[0m   )\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare the target data shifted by one time step\n",
    "target_train_shifted = np.zeros_like(target_train)\n",
    "target_val_shifted = np.zeros_like(target_val)\n",
    "\n",
    "target_train_shifted[:, :-1] = target_train[:, 1:]\n",
    "target_val_shifted[:, :-1] = target_val[:, 1:]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [input_train, target_train], target_train_shifted,\n",
    "    batch_size=8,\n",
    "    epochs=1,\n",
    "    validation_data=([input_val, target_val], target_val_shifted)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict([input_test, target_test])\n",
    "\n",
    "# Flatten the predictions and true values\n",
    "preds_flat = np.argmax(predictions, axis=-1).flatten()\n",
    "targets_flat = target_test.flatten()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(targets_flat, preds_flat)\n",
    "precision = precision_score(targets_flat, preds_flat, average='weighted')\n",
    "recall = recall_score(targets_flat, preds_flat, average='weighted')\n",
    "f1 = f1_score(targets_flat, preds_flat, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f00719",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(128,))\n",
    "decoder_state_input_c = Input(shape=(128,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text):\n",
    "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_length, padding='post')\n",
    "    \n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer.word_index['<start>']\n",
    "    \n",
    "    generated_response = ''\n",
    "    \n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = tokenizer.index_word.get(sampled_token_index, '<unk>')\n",
    "        \n",
    "        if sampled_word == '<end>' or len(generated_response.split()) >= max_length:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            generated_response += ' ' + sampled_word\n",
    "        \n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return generated_response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa269fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"bye\":\n",
    "        response, _ = handle_input(user_input)\n",
    "        print(\"Bot:\", response)\n",
    "        break\n",
    "\n",
    "    response, entities = handle_input(user_input)\n",
    "    print(\"Bot:\", response)\n",
    "    print('Entities:', entities)\n",
    "    print(\"Context:\", get_context())\n",
    "    print('\\n'+'='*50+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37866899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
